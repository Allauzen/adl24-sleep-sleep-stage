{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  as th\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Model\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomProjection:\n",
    "    def __init__(self, input_dim, quantizer_dim, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.input_dim = input_dim\n",
    "        self.quantizer_dim = quantizer_dim\n",
    "        self.projection_matrix = None\n",
    "        self.codebook = None\n",
    "\n",
    "    def initialize_quantizer(self):\n",
    "        # Initialize projection matrix with Xavier initialization\n",
    "        self.projection_matrix = nn.init.xavier_uniform_(th.empty(self.quantizer_dim, self.input_dim))\n",
    "\n",
    "        # Initialize codebook with standard normal distribution\n",
    "        self.codebook = th.randn(self.input_dim // 10, self.quantizer_dim)\n",
    "\n",
    "    def project(self, X):\n",
    "        th.manual_seed(self.random_state)\n",
    "\n",
    "        self.initialize_quantizer()\n",
    "\n",
    "        #Normalize data\n",
    "        X_normalized = X / th.norm(X, dim=1, keepdim=True)\n",
    "\n",
    "        # Project input data using projection matrix\n",
    "        X_projected = th.matmul(X_normalized, self.projection_matrix.T)\n",
    "        return X_projected\n",
    "\n",
    "    def quantize(self, X):\n",
    "\n",
    "        X_projected = self.project(X)\n",
    "        expanded_codebook = codebook.repeat(X_projected.shape[0],1,1)\n",
    "\n",
    "        # Compute distances to codebook vectors\n",
    "        distances = th.norm(th.norm(X_projected) - th.norm(self.codebook), dim=2)\n",
    "\n",
    "        # Find nearest vector index\n",
    "        labels = th.argmin(distances, dim=1)\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "class ASREncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=256, num_layers=5, dropout=0.1):\n",
    "        super(ASREncoder, self).__init__()\n",
    "\n",
    "        # Feature extractor (Wav2Vec2 model)\n",
    "        self.feature_extractor = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.feature_extractor.freeze_feature_extractor()  # Freeze feature extractor\n",
    "\n",
    "        # Conformer layers\n",
    "        conformer_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            conformer_layers.append(nn.TransformerEncoderLayer(\n",
    "                d_model=input_dim,\n",
    "                nhead=4,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                dropout=dropout,\n",
    "                activation='gelu'\n",
    "            ))\n",
    "        self.conformer_encoder = nn.TransformerEncoder(nn.ModuleList(conformer_layers), num_layers=num_layers)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the pre-trained Wav2Vec2 model\n",
    "        features = self.feature_extractor(x).last_hidden_state\n",
    "\n",
    "        # Transpose features for Transformer input format (seq_len, batch_size, hidden_dim)\n",
    "        features = features.permute(1, 0, 2)\n",
    "\n",
    "        # Apply Conformer layers\n",
    "        features = self.conformer_encoder(features)\n",
    "\n",
    "        # Average pooling over time\n",
    "        pooled_features = torch.mean(features, dim=0)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(pooled_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SelfSupervisedRandomProjectionQuantizer:\n",
    "    def __init__(self, input_dim, quantizer_dim, num_classes, hidden_dim, batch_size):\n",
    "        self.quantizer = RandomProjection(input_dim, quantizer_dim)\n",
    "        self.asr_encoder = ASREncoder(quantizer_dim, hidden_dim, num_classes)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = None\n",
    "\n",
    "    def initialize_optimizer(self, learning_rate):\n",
    "        self.optimizer = optim.Adam(self.asr_encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def split_batch(self, X, y):\n",
    "        dataset = TensorDataset(X, y)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return data_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pretrain(self, xtrain, ytrain, xvalid, yvalid, epochs=10, batch_size=32):\n",
    "        # Initialize quantizer\n",
    "        self.quantizer.initialize_quantizer()\n",
    "        trainloader = self.splitbatch(xtrain, ytrain)\n",
    "        validloader = self.splitbatch(xvalid, yvalid)\n",
    "\n",
    "        # Iterate over epochs\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for inputs, labels in trainloader:\n",
    "\n",
    "                # Zero gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                labels = self.quantizer.quantize(inputs,)\n",
    "                predictions = self.asr_encoder(labels)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss_function(predictions, mini_batch_y)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Print epoch loss\n",
    "            print(f\"Pretraining Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(X_tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data/cassette-th-data.pck'\n",
    "xtrain, xvalid, ytrain, yvalid = np.load(filepath, allow_pickle = True)\n",
    "input_dim = xtrain[0].shape\n",
    "input_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codebook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tmp \u001b[38;5;241m=\u001b[39m RandomProjection(input_dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m, quantizer_dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m23\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mRandomProjection.quantize\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     30\u001b[0m     X_projected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(X)\n\u001b[0;32m---> 31\u001b[0m     expanded_codebook \u001b[38;5;241m=\u001b[39m \u001b[43mcodebook\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat(X_projected\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Compute distances to codebook vectors\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     distances \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mnorm(th\u001b[38;5;241m.\u001b[39mnorm(X_projected) \u001b[38;5;241m-\u001b[39m th\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codebook' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = RandomProjection(input_dim= 600, quantizer_dim= 100, random_state= 23)\n",
    "tmp.quantize(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/constouille/Library/Caches/pypoetry/virtualenvs/dl-sleep-project-wfnrCJ-y-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ASREncoder(\n",
       "  (feature_extractor): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x ModuleList(\n",
       "        (0-4): 5 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=8, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=8, bias=True)\n",
       "          (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=8, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = ASREncoder(8, 4)\n",
    "tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/constouille/Library/Caches/pypoetry/virtualenvs/dl-sleep-project-wfnrCJ-y-py3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1471: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n",
      "/Users/constouille/Library/Caches/pypoetry/virtualenvs/dl-sleep-project-wfnrCJ-y-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "test = SelfSupervisedRandomProjectionQuantizer(input_dim= 600, quantizer_dim = 100, hidden_dim= 10, num_classes=5, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 items not taken\n"
     ]
    }
   ],
   "source": [
    "mini_batchs = test.split_batch(xtrain)\n",
    "trainloader = DataLoader(mini_batchs, batch_size = 200, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 items not taken\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 100\u001b[0m, in \u001b[0;36mSelfSupervisedRandomProjectionQuantizer.split_batch\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     98\u001b[0m rest \u001b[38;5;241m=\u001b[39m shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items not taken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnumber_of_batch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(number_of_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "test.split_batch(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(xtrain, ytrain)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 100  # Choose your desired batch size\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  # You can set shuffle=True to shuffle the data during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.2484e+00, -2.4982e-01,  3.7492e+00,  ...,  7.7482e+00,\n",
       "           9.2479e+00,  2.0245e+01],\n",
       "         [-1.2451e+01, -1.2903e+01, -1.2903e+01,  ..., -7.0176e+00,\n",
       "          -2.2637e-01,  5.6593e+00],\n",
       "         [ 2.8723e+02,  2.1895e+02,  1.4922e+02,  ..., -7.1949e+00,\n",
       "          -1.8333e+01, -5.2230e+01],\n",
       "         ...,\n",
       "         [ 2.5690e-01, -2.6603e+00, -3.6327e+00,  ...,  8.5223e+00,\n",
       "           3.1741e+00,  2.6879e+00],\n",
       "         [-7.9414e-01,  6.1758e-01,  2.9705e+00,  ..., -1.3500e+01,\n",
       "          -9.7350e+00, -1.8676e+01],\n",
       "         [ 9.9558e+00,  9.5160e+00,  1.6113e+01,  ...,  1.8312e+01,\n",
       "           5.1179e+00, -5.9951e-01]]),\n",
       " tensor([2, 2, 0, 1, 0, 1, 0, 2, 1, 4, 1, 2, 1, 2, 0, 2, 0, 2, 2, 2, 1, 2, 2, 2,\n",
       "         2, 4, 0, 0, 3, 2, 2, 4, 2, 4, 1, 4, 0, 0, 4, 2, 0, 2, 0, 2, 4, 2, 2, 1,\n",
       "         2, 2, 4, 1, 2, 1, 0, 2, 2, 2, 3, 0, 2, 1, 2, 0, 3, 2, 0, 0, 0, 0, 2, 2,\n",
       "         1, 0, 2, 2, 1, 0, 0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 4, 1],\n",
       "        dtype=torch.int32))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inputs, labels in data_loader:\n",
    "    a = 1\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([191.7634, 191.6759, 191.7279, 191.8734, 192.4562, 191.9494, 192.5126,\n",
       "        193.1053, 192.0806, 191.3367, 192.6963, 192.1805, 192.6700, 191.8028,\n",
       "        191.7176, 191.1822, 191.8556, 192.2178, 192.5416, 191.4962, 190.9630,\n",
       "        192.2687, 191.8800, 191.9859, 192.4312, 191.9797, 193.7579, 191.1921,\n",
       "        192.7931, 192.0800, 192.2573, 191.1365, 191.9663, 191.7607, 192.5780,\n",
       "        192.9385, 191.2001, 191.8400, 192.1129, 192.2557, 191.3062, 192.0537,\n",
       "        191.7319, 191.1817, 192.3630, 192.1825, 193.0748, 191.0130, 192.0197,\n",
       "        191.8678, 190.7955, 191.7078, 192.3392, 192.4276, 193.3549, 193.8392,\n",
       "        191.8078, 192.5172, 191.7083, 192.0896])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = tmp.project(xtrain)\n",
    "codebook = tmp.codebook\n",
    "th.norm(xtrain[0]) - th.norm(codebook, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (200) must match the size of tensor b (60) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m codebook \u001b[38;5;241m=\u001b[39m codebook\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (200) must match the size of tensor b (60) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "codebook = codebook.repeat(200, 1,1)\n",
    "th.norm(xtrain[:200], dim = 1) - th.norm(codebook, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = th.norm(xtrain[:200], dim = 1)\n",
    "b = th.norm(codebook, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.argmin(a -  th.transpose(th.norm(codebook, dim = 2), 0, 1), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.argmin(a - th.transpose(b, 0, 1), dim =0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-sleep-project-wfnrCJ-y-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
