{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch  as th\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Model\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Set the device to use Apple M1 GPU #######\n",
    "device = th.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomProjection:\n",
    "    def __init__(self, input_dim, quantizer_dim, codebook_size, random_state=None, device = 'cpu'):\n",
    "        self.random_state = random_state\n",
    "        self.input_dim = input_dim\n",
    "        self.quantizer_dim = quantizer_dim\n",
    "        self.codebook_size = codebook_size\n",
    "        self.projection_matrix = None\n",
    "        self.codebook = None\n",
    "\n",
    "    def initialize_quantizer(self):\n",
    "        # Initialize projection matrix with Xavier initialization\n",
    "        self.projection_matrix = nn.init.xavier_uniform_(th.empty(self.quantizer_dim, self.input_dim)).to(device)\n",
    "\n",
    "        # Initialize codebook with standard normal distribution\n",
    "        self.codebook = nn.init.xavier_normal_(th.empty(self.codebook_size, self.quantizer_dim), gain=1.0).to(device)\n",
    "\n",
    "    def project(self, X):\n",
    "        th.manual_seed(self.random_state)\n",
    "\n",
    "        self.initialize_quantizer()\n",
    "\n",
    "        #Normalize data\n",
    "        X_normalized = X / th.norm(X, dim=1, keepdim=True)\n",
    "\n",
    "        print(self.projection_matrix.shape)\n",
    "\n",
    "        # Project input data using projection matrix\n",
    "        X_projected = th.matmul(X_normalized, self.projection_matrix.T)\n",
    "\n",
    "        return X_projected\n",
    "\n",
    "    def quantize(self, X):\n",
    "\n",
    "        X_projected = self.project(X)\n",
    "\n",
    "        print(X_projected.shape)\n",
    "\n",
    "        norm_x = th.norm(X_projected, dim = 1)\n",
    "\n",
    "\n",
    "        codebook = self.codebook.repeat(X_projected.shape[0], 1, 1)\n",
    "\n",
    "        norm_codebook = th.norm(codebook, dim = 1)\n",
    "\n",
    "        # Compute distances to codebook vectors\n",
    "        distances = norm_x - th.transpose(norm_codebook, 0, 1)\n",
    "\n",
    "        # Find nearest vector index\n",
    "        labels = th.argmin(distances, dim=0)\n",
    "\n",
    "        print(norm_x.shape, norm_codebook.shape, distances.shape)\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "class ASREncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=256, num_layers=5, dropout=0.1):\n",
    "        super(ASREncoder, self).__init__()\n",
    "\n",
    "        # Feature extractor (Wav2Vec2 model)\n",
    "        self.feature_extractor = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.feature_extractor.freeze_feature_extractor()  # Freeze feature extractor\n",
    "\n",
    "        # Conformer layers\n",
    "        conformer_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            conformer_layers.append(nn.TransformerEncoderLayer(\n",
    "                d_model=input_dim,\n",
    "                nhead=4,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                dropout=dropout,\n",
    "                activation='gelu'\n",
    "            ))\n",
    "        self.conformer_encoder = nn.TransformerEncoder(nn.ModuleList(conformer_layers), num_layers=num_layers)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the pre-trained Wav2Vec2 model\n",
    "        features = self.feature_extractor(x).last_hidden_state\n",
    "\n",
    "        # Transpose features for Transformer input format (seq_len, batch_size, hidden_dim)\n",
    "        features = features.permute(1, 0, 2)\n",
    "\n",
    "        # Apply Conformer layers\n",
    "        features = self.conformer_encoder(features)\n",
    "\n",
    "        # Average pooling over time\n",
    "        pooled_features = torch.mean(features, dim=0)\n",
    "\n",
    "        # Classification\n",
    "        output = self.classifier(pooled_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "class BESTRq_framework:\n",
    "    def __init__(self, input_dim, quantizer_dim, num_classes, hidden_dim, batch_size, codebook_size, num_time_steps, mask_prob):\n",
    "        self.quantizer = RandomProjection(input_dim, quantizer_dim, codebook_size)\n",
    "        self.asr_encoder = ASREncoder(quantizer_dim, hidden_dim, num_classes)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.mask_prob = mask_prob\n",
    "        self.optimizer = None\n",
    "\n",
    "    def initialize_optimizer(self, learning_rate):\n",
    "        self.optimizer = optim.Adam(self.asr_encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def split_batch(self, X, y):\n",
    "        dataset = TensorDataset(X, y)\n",
    "        data_loader = DataLoader(dataset, batch_size= self.batch_size, shuffle=True)\n",
    "        return data_loader\n",
    "\n",
    "    def masking(self, input_values: th.Tensor, input_lengths: th.Tensor) -> tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_values (torch.Tensor): with shape `(B, L, D)`\n",
    "            input_lengths (torch.Tensor): with shape `(B)'\n",
    "\n",
    "        Returns:\n",
    "            tuple(\n",
    "            torch.Tensor with shape `(B, L, D)`\n",
    "            torch.Tensor with shape `(B, L)`\n",
    "            )\n",
    "        \"\"\"\n",
    "        batch_size, num_steps, hidden_size = input_values.size()\n",
    "\n",
    "        # non mask: 0, maks: 1\n",
    "        time_mask_indices = th.zeros(\n",
    "            batch_size, num_steps + self.num_time_steps,\n",
    "            device=input_values.device, dtype=th.bool\n",
    "        )\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            time_mask_idx_candidates = list(range(int(input_lengths[batch])))\n",
    "            k = int(self.mask_prob * input_lengths[batch])\n",
    "            start_time_mask_idx_array = th.tensor(\n",
    "                random.sample(time_mask_idx_candidates, k=k), device=input_values.device, dtype=th.long\n",
    "            )\n",
    "\n",
    "            for i in range(self.num_time_steps):\n",
    "                time_mask_indices[batch, start_time_mask_idx_array+i] = 1\n",
    "\n",
    "        time_mask_indices = time_mask_indices[:, :-self.num_time_steps]\n",
    "        num_masks = sum(time_mask_indices.flatten())\n",
    "\n",
    "        # Replace to random value where mask\n",
    "        random_values = th.normal(mean=0, std=0.1, size=(num_masks, hidden_size), device=input_values.device)\n",
    "        input_values[time_mask_indices == 1] = random_values\n",
    "\n",
    "        return input_values, time_mask_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def pretrain(self, xtrain, ytrain, xvalid, yvalid, epochs=10, batch_size=32):\n",
    "        # Initialize quantizer\n",
    "        self.quantizer.initialize_quantizer()\n",
    "        trainloader = self.splitbatch(xtrain, ytrain)\n",
    "        validloader = self.splitbatch(xvalid, yvalid)\n",
    "\n",
    "        # Iterate over epochs\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for inputs, labels in trainloader:\n",
    "\n",
    "                # Zero gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                labels = self.quantizer.quantize(inputs)\n",
    "                predictions = self.asr_encoder(labels)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = self.loss_function(predictions, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Print epoch loss\n",
    "            print(f\"Pretraining Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(X_tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'data/cassette-th-data.pck'\n",
    "xtrain, xvalid, ytrain, yvalid = np.load(filepath, allow_pickle = True)\n",
    "input_dim = xtrain[0].shape\n",
    "input_dim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 600])\n",
      "torch.Size([175995, 100])\n",
      "torch.Size([175995]) torch.Size([175995, 100]) torch.Size([100, 175995])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([81, 81, 81,  ..., 81, 81, 81], device='mps:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = RandomProjection(input_dim= 600, quantizer_dim= 100, codebook_size= 20,random_state= 2673834267383, device = device)\n",
    "tmp.quantize(xtrain.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467, 467,\n",
       "        467, 467, 467, 467, 467, 467])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = RandomProjection(input_dim= 600, quantizer_dim= 500, codebook_size= 1000,random_state= 16722)\n",
    "tmp.quantize(xtrain[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/constouille/Library/Caches/pypoetry/virtualenvs/dl-sleep-project-wfnrCJ-y-py3.10/lib/python3.10/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1471: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n",
      "/Users/constouille/Library/Caches/pypoetry/virtualenvs/dl-sleep-project-wfnrCJ-y-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "test = SelfSupervisedRandomProjectionQuantizer(input_dim= 600, quantizer_dim = 100, hidden_dim= 10, num_classes=5, batch_size=200, codebook_size= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2b116ec20>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batchs = test.split_batch(xtrain, ytrain)\n",
    "mini_batchs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-sleep-project-wfnrCJ-y-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
